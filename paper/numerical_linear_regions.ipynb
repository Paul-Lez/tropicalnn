{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSetNetwork(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DeepSetNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, input_dim)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        alpha, gamma = np.random.rand() * 2 - 1, np.random.rand() * 2 - 1\n",
    "        weights = alpha * torch.eye(self.linear1.weight.shape[0]) + gamma * (torch.ones_like(self.linear1.weight))\n",
    "        self.linear1.weight.data = weights\n",
    "        self.linear1.bias.data = torch.zeros_like(self.linear1.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = torch.sum(x, dim=1)\n",
    "        return x.view(-1, 1)\n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dims,init_fn=None):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(dims) - 1):\n",
    "            layers.append(nn.Linear(dims[i], dims[i + 1]))\n",
    "            if i < len(dims) - 2: \n",
    "                layers.append(nn.ReLU())\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        if init_fn:\n",
    "            self.apply(init_fn)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "def random_mlp_init(m):\n",
    "    if isinstance(m,nn.Linear):\n",
    "        weights_size=(m.weight.data).shape\n",
    "        m.weight.data=torch.normal(0,np.sqrt(2/weights_size[1])*torch.ones(weights_size[0],weights_size[1]))\n",
    "        bias_length=len(m.bias.data)\n",
    "        m.bias.data=torch.normal(0,np.sqrt(2/bias_length)*torch.ones(bias_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_cube(npoints, ndim, side_length):\n",
    "    vec = np.random.uniform(-side_length, side_length, (npoints, ndim))\n",
    "    return torch.Tensor(vec)\n",
    "\n",
    "def get_jacobians_with_outputs(points, model):\n",
    "    points.requires_grad = True\n",
    "    output = model(points)\n",
    "    output.backward(torch.ones_like(output))\n",
    "    jacobians = points.grad\n",
    "    return jacobians,output\n",
    "\n",
    "def remove_duplicates(jacobians,points,outputs,model,midpoint_sampling=2):\n",
    "    new_jacobians=np.expand_dims(jacobians[0,:],0)\n",
    "    new_points=np.expand_dims(points[0,:],0)\n",
    "    new_outputs=np.expand_dims(outputs[0,:],0)\n",
    "\n",
    "    for k in range(1,jacobians.shape[0]):\n",
    "        jacobian=jacobians[k,:]\n",
    "        point=points[k,:]\n",
    "        output=outputs[k,:]\n",
    "\n",
    "        differences=new_jacobians-jacobian\n",
    "        candidates=np.where(np.linalg.norm(differences,axis=1)==0)[0]\n",
    "        if len(candidates)==0:\n",
    "            new_jacobians=np.concatenate([new_jacobians,np.expand_dims(jacobian,0)])\n",
    "            new_points=np.concatenate([new_points,np.expand_dims(point,0)])\n",
    "            new_outputs=np.concatenate([new_outputs,np.expand_dims(output,0)])\n",
    "        else:\n",
    "            for m in range(1,midpoint_sampling):\n",
    "\n",
    "                midpoints=new_points[candidates,:]+(point-new_points[candidates,:])*m/midpoint_sampling\n",
    "                model_midpoints=model(torch.tensor(midpoints)).detach().numpy()\n",
    "                linear_map_midpoints=new_outputs[candidates,:]+(output-new_outputs[candidates,:])*m/midpoint_sampling\n",
    "                \n",
    "                updated_candidates=np.where(np.abs(model_midpoints-linear_map_midpoints)<1e-4)[0]\n",
    "                if len(updated_candidates)==0:\n",
    "                    new_jacobians=np.concatenate([new_jacobians,np.expand_dims(jacobian,0)])\n",
    "                    new_points=np.concatenate([new_points,np.expand_dims(point,0)])\n",
    "                    new_outputs=np.concatenate([new_outputs,np.expand_dims(output,0)])\n",
    "                    break\n",
    "    return new_jacobians\n",
    "\n",
    "def sort_point_components(tensor):\n",
    "    return torch.sort(tensor, dim=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_linear_regions(model,radius,n_points,point_dim):\n",
    "    points = sample_from_cube(npoints=n_points, ndim=point_dim, side_length=radius)\n",
    "    jacobians,outputs = get_jacobians_with_outputs(points, model)\n",
    "    jacobians=jacobians.detach().numpy()\n",
    "    outputs=outputs.detach().numpy()\n",
    "    points=points.detach().numpy()\n",
    "    unique_jacobians = remove_duplicates(np.around(jacobians, 10),points,outputs,model)\n",
    "    return unique_jacobians.shape[0]\n",
    "\n",
    "def estimate_linear_regions_using_fundamental_domain(model, radius, n_points, point_dim):\n",
    "    points = sample_from_cube(npoints=n_points, ndim=point_dim, side_length=radius)\n",
    "    points = sort_point_components(points)\n",
    "    jacobians,outputs = get_jacobians_with_outputs(points, model)\n",
    "    jacobians=jacobians.detach().numpy()\n",
    "    outputs=outputs.detach().numpy()\n",
    "    points=points.detach().numpy()\n",
    "    unique_jacobians = remove_duplicates(np.around(jacobians, 10),points,outputs,model)\n",
    "    total = 0\n",
    "    for jacobian in unique_jacobians:\n",
    "        counts = np.unique(jacobian, return_counts=True)[1]\n",
    "        total += math.factorial(point_dim) / np.prod([math.factorial(count) for count in counts])\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_fundamental_domain_experiment(num_models, point_dim, search_radius, base_num_points):\n",
    "    num_points_adjusted_for_dimension = base_num_points ** point_dim\n",
    "    print(f\"Running experiment with {num_models} models, {point_dim} input dimension, search radius {search_radius}, and {num_points_adjusted_for_dimension} points.\")\n",
    "    models = [DeepSetNetwork(point_dim) for _ in range(num_models)]\n",
    "    all_ratios = []\n",
    "    time_regular=[]\n",
    "    time_fundamental=[]\n",
    "    for i, model in enumerate(models):\n",
    "        start_time=time.time()\n",
    "        regular_sampling = estimate_linear_regions(model, search_radius, num_points_adjusted_for_dimension, point_dim)\n",
    "        time_regular.append(time.time()-start_time)\n",
    "        start_time=time.time()\n",
    "        fundamental_domain_sampling = estimate_linear_regions_using_fundamental_domain(model, search_radius, num_points_adjusted_for_dimension//(math.factorial(point_dim)), point_dim)\n",
    "        time_fundamental.append(time.time()-start_time)\n",
    "        ratio = fundamental_domain_sampling / regular_sampling\n",
    "        all_ratios.append(ratio)\n",
    "\n",
    "    average_ratio = sum(all_ratios) / num_models\n",
    "    print(f\"Average ratio: {average_ratio} Time: {sum(time_regular)/num_models} Time Fundamental: {sum(time_fundamental)/num_models}\")\n",
    "\n",
    "    return all_ratios,time_regular,time_fundamental\n",
    "\n",
    "def count_linear_regions(architectures, search_radius=20, num_points=2000, nn_samples=5, midpoint_sampling=2):\n",
    "    architecture_avg_counts=[]\n",
    "    architecture_avg_times=[]\n",
    "    architecture_avg_samples=[]\n",
    "    for architecture in architectures:\n",
    "        num_lin_regions=[]\n",
    "        times=[]\n",
    "        number_of_samples=[]\n",
    "        point_dim=architecture[0]\n",
    "        for sample_num in range(nn_samples):\n",
    "            model=MLP(architecture,init_fn=random_mlp_init)\n",
    "            points=sample_from_cube(npoints=num_points, ndim=point_dim, side_length=search_radius)\n",
    "            start_time=time.time()\n",
    "            jacobians,outputs=get_jacobians_with_outputs(points, model)\n",
    "            jacobians=jacobians.detach().numpy()\n",
    "            outputs=outputs.detach().numpy()\n",
    "            unique_jacobians,counts=remove_duplicates(np.around(jacobians, 10),points.detach().numpy(),outputs,model,midpoint_sampling=midpoint_sampling)\n",
    "            total_time=time.time()-start_time\n",
    "            num_lin_regions.append(unique_jacobians)\n",
    "            times.append(total_time)\n",
    "            number_of_samples.append(num_points+counts)\n",
    "        architecture_avg_counts.append(sum(num_lin_regions)/len(num_lin_regions))\n",
    "        architecture_avg_times.append(sum(times)/len(times))\n",
    "        architecture_avg_samples.append(sum(number_of_samples)/len(number_of_samples))\n",
    "        \n",
    "        print(f\"{architecture} num regions = {architecture_avg_counts[-1]}, time = {architecture_avg_times[-1]}, samples = {architecture_avg_samples[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_MODELS = 10\n",
    "point_dim_to_ratios = {}\n",
    "for point_dim in range(2, 7):\n",
    "    ratios,times_regular,times_fundamental=sampling_fundamental_domain_experiment(num_models=NUM_MODELS, point_dim=point_dim, search_radius=20, base_num_points=10)\n",
    "    point_dim_to_ratios[point_dim] = {\"ratios\":ratios,\"times_regular\":times_regular,\"time_fundamental\":times_fundamental}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_SAMPLES=25\n",
    "ARCHITECTURES=[[2,6,1],[3,5,1],[4,4,1],[5,3,1],[6,2,1],[3,2,2,1],[3,3,2,1]]\n",
    "\n",
    "search_radius=5\n",
    "num_points=1000\n",
    "print(f\"search radius {search_radius}, num_points {num_points}, nn_samples {NN_SAMPLES}\")\n",
    "count_linear_regions(ARCHITECTURES,search_radius=search_radius,num_points=num_points,nn_samples=NN_SAMPLES)\n",
    "\n",
    "search_radius=20\n",
    "num_points=1000\n",
    "print(f\"search radius {search_radius}, num_points {num_points}, nn_samples {NN_SAMPLES}\")\n",
    "count_linear_regions(ARCHITECTURES,search_radius=search_radius,num_points=num_points,nn_samples=NN_SAMPLES)\n",
    "\n",
    "search_radius=5\n",
    "num_points=5000\n",
    "print(f\"search radius {search_radius}, num_points {num_points}, nn_samples {NN_SAMPLES}\")\n",
    "count_linear_regions(ARCHITECTURES,search_radius=search_radius,num_points=num_points,nn_samples=NN_SAMPLES)\n",
    "\n",
    "search_radius=20\n",
    "num_points=5000\n",
    "print(f\"search radius {search_radius}, num_points {num_points}, nn_samples {NN_SAMPLES}\")\n",
    "count_linear_regions(ARCHITECTURES,search_radius=search_radius,num_points=num_points,nn_samples=NN_SAMPLES)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
